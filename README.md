# Egoshots
Egoshots: A 2-month Ego-vision Dataset with Autographer Wearable Camera annotated "for free" with transfer learning. 
Three state of the art pre-trained image captioning models are used.

The dataset represents the life of 2 interns while working at Philips Research (Netherlands) (May-July 2015) generously donating their data:

* Natalia Díaz Rodríguez 
* Vana Panagiotou

See associated baselines, Semantic Fidelity metric and documentation in the associated paper:

and repo:

https://github.com/Pranav21091996/Semantic_Fidelity-and-Egoshots

For research ideas, experiments and collaboration with the full dataset -this repo only includes a subset of 978 images- (including GPS, ambient light, accelerometer, magnetometer, PIR, temperature), please contact natalia.diaz@ensta-paris.fr



---
Credit-thanks to Alejandro Betancourt for the Autographer camera and all rest of open-minded Eindhoven friends for being part of Vana and my story :)

# Paper associated

If you use it, please cite:

Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models Pranav Agarwal, Alejandro Betancourt, Vana Panagiotou, Natalia Díaz-Rodríguez. Machine Learning in Real Life (ML-IRL) ICLR 2020 Workshop https://arxiv.org/abs/2003.11743

```
@InProceedings{Agarwal20egoshots,
    author={Pranav Agarwal and Alejandro Betancourt and Vana Panagiotou and Natalia Díaz-Rodríguez},
    year={2020},
    month = {Mar},
    booktitle = {Machine Learning in Real Life (ML-IRL) Workshop at the International Conference on Learning Representations  (ICLR)},
    url={https://arxiv.org/abs/2003.11743}
}
```

